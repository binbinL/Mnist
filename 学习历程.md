## 学习历程

在学习这个方向的的初始阶段，对这方面的知识一无所知，不懂如何下手。首先将书大致浏览了一遍，了解了一些相关概念。在有一定的知识储备后看视频，比较好的理解实现过程。

在看完视频后，我对任务似乎有了点想法，要搭建一个784-->n[1]-->n[2]...-->10的一个神经网络。

任务是手写数字识别，使用keras框架来完成。

### 数据预处理

```python
import numpy as np
from keras.optimizers import RMSprop
from tensorflow.python.keras.utils.np_utils import to_categorical
from keras import models,layers,regularizers
from matplotlib import pyplot as plt
```



```python
#先将训练集和测试集载入
train_page = np.loadtxt('mnist_train.csv' , delimiter="," , dtype='float', skiprows=1)
test_page = np.loadtxt('mnist_test.csv' , delimiter="," , dtype='float', skiprows=1)
#训练集和测试集的特征值提取
train_im = train_page[:,1:]
test_im = test_page[:,1:]
#训练集和测试集的标签提取
train_label = to_categorical(train_page[:,0])
test_label = to_categorical(test_page[:,0])
```

其中to_categorical的作用是将向量转为只含0 1的矩阵，这样做更好对应每组数据的对应标签：

```python
from tensorflow.python.keras.utils.np_utils import to_categorical
b = [0,1,2,3,4,5,6,7,8]
b = to_categorical(b, 9)
print(b)
#结果
[[1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 1. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 1.]]
```



###神经网络搭建

输入层是m组数据，每组数据有28*28个特征值，输出层有10个点。

在这里我是搭建一个两层的隐藏层，激活函数都是’relu‘

**过拟合表现为训练数据的准确率高于测试数据的准确率**

​	*使用 layers.Dropout( )和正则化能够减少过拟合*

```python
network = models.Sequential()
#200个神经结点，激活函数选择relu，并且正则化，选择l1，为了减少模型过拟合
network.add(layers.Dense(units=200 , activation='relu' , input_shape=(28*28, ) ,
                         kernel_regularizer=regularizers.l1(0.0002) ))
#也是防止过拟合：在每次训练时随机忽略一部分神经元
network.add(layers.Dropout(0.05))
network.add(layers.Dense(units=100, activation='relu',
                         kernel_regularizer=regularizers.l1(0.0001)))
network.add(layers.Dropout(0.05))
#10个神经结点，对应输出，激活函数选择softmax
network.add(layers.Dense(units=10 , activation='softmax'))
```



### 编译以及训练

```python
#learning rate：0.001 
#loss function：categorical_crossentropy
network.compile(optimizer = RMSprop(lr = 0.001) , loss='categorical_crossentropy' , metrics=['accuracy'])
#optimizer：SGD RMSSprop Adagrad Adam Adamax Nadam

#训练网络epochs表示训练多少个回合， batch_size表示每次训练给多大的数据
#verbose不同值控制控制台的输出 (2 ：为每个epoch输出一行记录
network.fit(train_im, train_label, epochs=25, batch_size=128, verbose=2)
```



### 模型评估

```python
plt.plot(np.arange(len(history.history['loss'])),
         						history.history['loss'],label='training')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend(loc=0)
plt.show()
#用evaluate函数，将测试数据以及测试结果传入
test_loss , test_accuracy = network.evaluate(test_im,test_label)
print("test_loss:", test_loss, "  test_accuracy:", test_accuracy)
```



### 模型保存

```python
network.save('./MIN_model.h5')
```

### 模型运用

这里遇到一个坑，一开始使用csv中的方法存数据，无法存28000个，百度后也无果，后来尝试换了pandas中的方法，便可行。。

```python
import numpy as np
import tensorflow as tf
import pandas as pd
test_page = np.loadtxt('test.csv' , delimiter="," , dtype='float', skiprows=1)
model = tf.keras.models.load_model('./MIN_model.h5')
prediction = model.predict(test_page[:])
result = np.argmax(prediction,axis = 1)#选取概率最大的数值
a = []
for i in range(1,28001):
    a.append(i)
dataframe = pd.DataFrame({'ImageId':a,'Label':result})
dataframe.to_csv("rre.csv",index=False,sep=',')
```



### 参考

[keras]（https://keras.io/zh/）。

[学习网站]（https://www.bilibili.com/）。